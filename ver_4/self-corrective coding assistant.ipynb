{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain_comunity (from versions: none)\n",
      "ERROR: No matching distribution found for langchain_comunity\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_comunity tiktoken langchainhub chromadb langchain langgraph faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain Expression Language (LCEL) | ğŸ¦œï¸�ğŸ”— LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ› ï¸� LangSmithğŸ¦œğŸ•¸ï¸�LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\n",
      "LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€� chain to the most complex chains (weâ€™ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\n",
      "When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\n",
      "Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\n",
      "Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\n",
      "Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. Weâ€™re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\n",
      "For more complex chains itâ€™s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and itâ€™s available on every LangServe server.Input and output schemas\n",
      "Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\n",
      "As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\n",
      "With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\n",
      "Any chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from langchain_community.llms import Anyscale\n",
    "# model = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "#os.environ[\"ANYSCALE_API_BASE\"] = \"https://api.endpoints.anyscale.com/v1\"\n",
    "#os.environ[\"ANYSCALE_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatAnyscale\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "## Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "## LLM\n",
    "model = ChatOpenAI(temperature=0, model=model, streaming=True)\n",
    "\n",
    "# Tool\n",
    "code_tool_oai = convert_to_openai_tool(code)\n",
    "\n",
    "# LLM with tool and enforce invocation\n",
    "llm_with_tool = model.bind(\n",
    "    tools = [code_tool_oai],\n",
    "    #tools = [code],\n",
    "    tool_choice = {\"type\" : \"function\", \"function\" : {\"name\" : \"code\"}},\n",
    ")\n",
    "\n",
    "# Parser\n",
    "parser_tool = PydanticToolsParser(tools=[code])\n",
    "\n",
    "## Prompt\n",
    "template = \"\"\"\"You are a coding assistant with expertise in LCEL, LangChain express:\n",
    "    Here is a full set of LCEL documentation:\n",
    "    \\n ---------- \\n\n",
    "    {context}\n",
    "    \\n ---------- \\n\n",
    "    Answer the use question based on the above provided documentation. \\n\n",
    "    Ensure any code you provide can be executed with all required imports and variables defined. \\n\n",
    "    Structure your answer with a description of the problem and approach, followed by the code block. \\n\n",
    "    Then list the imports. And finally, the list function code block. \\n\n",
    "    Here is the user question: \\n ---------- \\n {question}\"\"\"\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'],\n",
    "    )\n",
    "\n",
    "# Chain\n",
    "chain = (\n",
    "    {\n",
    "        # \"context\": lambda x: docs,\n",
    "        'context': lambda x: concatenated_content,\n",
    "        'question': itemgetter('question'),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tool\n",
    "    | parser_tool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke({\"question\": \"How to create a RAG chain in LCEL?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"What is a LCEL?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[code(prefix='Creating a Retrieval-Augmented Generation (RAG) chain in LangChain Expression Language (LCEL) involves composing a chain that retrieves relevant documents and then generates a response based on those documents. The process typically involves the following steps: 1. Define the retriever to fetch relevant documents. 2. Define the LLM to generate a response based on the retrieved documents. 3. Compose these steps into a single chain. Below is an example of how to create a RAG chain in LCEL.', imports='from langchain import LCEL, Retriever, LLM', code='# Define the retriever step\\nretriever = Retriever(\\n    name=\"document_retriever\",\\n    params={\\n        \"index\": \"my_document_index\",\\n        \"query\": \"{input_query}\"\\n    }\\n)\\n\\n# Define the LLM step\\nllm = LLM(\\n    name=\"response_generator\",\\n    params={\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"prompt\": \"Based on the following documents: {retrieved_documents}, answer the question: {input_query}\"\\n    }\\n)\\n\\n# Compose the RAG chain\\nrag_chain = LCEL(\\n    steps=[\\n        retriever,\\n        llm\\n    ],\\n    input_schema={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"input_query\": {\"type\": \"string\"}\\n        },\\n        \"required\": [\"input_query\"]\\n    },\\n    output_schema={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"response\": {\"type\": \"string\"}\\n        }\\n    }\\n)\\n\\n# Example usage\\ninput_data = {\"input_query\": \"What is the capital of France?\"}\\nresponse = rag_chain.run(input_data)\\nprint(response)')]\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='Creating a Retrieval-Augmented Generation (RAG) chain in LangChain Expression Language (LCEL) involves composing a chain that retrieves relevant documents and then generates a response based on those documents. The process typically involves the following steps: 1. Define the retriever to fetch relevant documents. 2. Define the LLM to generate a response based on the retrieved documents. 3. Compose these steps into a single chain. Below is an example of how to create a RAG chain in LCEL.', imports='from langchain import LCEL, Retriever, LLM', code='# Define the retriever step\\nretriever = Retriever(\\n    name=\"document_retriever\",\\n    params={\\n        \"index\": \"my_document_index\",\\n        \"query\": \"{input_query}\"\\n    }\\n)\\n\\n# Define the LLM step\\nllm = LLM(\\n    name=\"response_generator\",\\n    params={\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"prompt\": \"Based on the following documents: {retrieved_documents}, answer the question: {input_query}\"\\n    }\\n)\\n\\n# Compose the RAG chain\\nrag_chain = LCEL(\\n    steps=[\\n        retriever,\\n        llm\\n    ],\\n    input_schema={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"input_query\": {\"type\": \"string\"}\\n        },\\n        \"required\": [\"input_query\"]\\n    },\\n    output_schema={\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"response\": {\"type\": \"string\"}\\n        }\\n    }\\n)\\n\\n# Example usage\\ninput_data = {\"input_query\": \"What is the capital of France?\"}\\nresponse = rag_chain.run(input_data)\\nprint(response)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define the retriever step\n",
      "retriever = Retriever(\n",
      "    name=\"document_retriever\",\n",
      "    params={\n",
      "        \"index\": \"my_document_index\",\n",
      "        \"query\": \"{input_query}\"\n",
      "    }\n",
      ")\n",
      "\n",
      "# Define the LLM step\n",
      "llm = LLM(\n",
      "    name=\"response_generator\",\n",
      "    params={\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"prompt\": \"Based on the following documents: {retrieved_documents}, answer the question: {input_query}\"\n",
      "    }\n",
      ")\n",
      "\n",
      "# Compose the RAG chain\n",
      "rag_chain = LCEL(\n",
      "    steps=[\n",
      "        retriever,\n",
      "        llm\n",
      "    ],\n",
      "    input_schema={\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "            \"input_query\": {\"type\": \"string\"}\n",
      "        },\n",
      "        \"required\": [\"input_query\"]\n",
      "    },\n",
      "    output_schema={\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "            \"response\": {\"type\": \"string\"}\n",
      "        }\n",
      "    }\n",
      ")\n",
      "\n",
      "# Example usage\n",
      "input_data = {\"input_query\": \"What is the capital of France?\"}\n",
      "response = rag_chain.run(input_data)\n",
      "print(response)\n"
     ]
    }
   ],
   "source": [
    "print(answer[0].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from langchain import LCEL, Retriever, LLM\n"
     ]
    }
   ],
   "source": [
    "print(answer[0].imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a Retrieval-Augmented Generation (RAG) chain in LangChain Expression Language (LCEL) involves composing a chain that retrieves relevant documents and then generates a response based on those documents. The process typically involves the following steps: 1. Define the retriever to fetch relevant documents. 2. Define the LLM to generate a response based on the retrieved documents. 3. Compose these steps into a single chain. Below is an example of how to create a RAG chain in LCEL.\n"
     ]
    }
   ],
   "source": [
    "print(answer[0].prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "    \n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keys': {'key1': 'value1', 'key2': 'value2'}}\n"
     ]
    }
   ],
   "source": [
    "a = GraphState(keys={\"key1\": \"value1\", \"key2\": \"value2\"})\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain_community.chat_models import ChatAnyscale\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate a code solution based on LCEL docs and the input question \n",
    "    with optional feedback from code execution tests \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    ## State\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    \n",
    "    ## Context \n",
    "    # LCEL docs\n",
    "    url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "    url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs_pydantic = loader.load()\n",
    "    \n",
    "    # LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "    url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs_sq = loader.load()\n",
    "    \n",
    "    # Add \n",
    "    docs.extend([*docs_pydantic, *docs_sq])\n",
    "    \n",
    "    # Sort the list based on the URLs in 'metadata' -> 'source'\n",
    "    d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "    d_reversed = list(reversed(d_sorted))\n",
    "    \n",
    "    # Concatenate the 'page_content' of each sorted dictionary\n",
    "    concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "        [doc.page_content for doc in d_reversed]\n",
    "    )\n",
    "    \n",
    "    ## Data model\n",
    "    class code(BaseModel):\n",
    "        \"\"\"Code output\"\"\"\n",
    "        prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "        imports: str = Field(description=\"Code block import statements\")\n",
    "        code: str = Field(description=\"Code block not including import statements\")\n",
    "    \n",
    "    ## LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "    \n",
    "    # Tool\n",
    "    code_tool_oai = convert_to_openai_tool(code)\n",
    "    \n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[code_tool_oai],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"code\"}},\n",
    "    )\n",
    "    \n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[code])\n",
    "    \n",
    "    ## Prompt\n",
    "    template = \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "        Here is a full set of LCEL documentation: \n",
    "        \\n ------- \\n\n",
    "        {context} \n",
    "        \\n ------- \\n\n",
    "        Answer the user question based on the above provided documentation. \\n\n",
    "        Ensure any code you provide can be executed with all required imports and variables defined. \\n\n",
    "        Structure your answer with a description of the code solution. \\n\n",
    "        Then list the imports. And finally list the functioning code block. \\n\n",
    "        Here is the user question: \\n --- --- --- \\n {question}\"\"\"\n",
    "\n",
    "    ## Generation\n",
    "    if \"error\" in state_dict:\n",
    "        print(\"---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\")\n",
    "        \n",
    "        error = state_dict[\"error\"]\n",
    "        code_solution = state_dict[\"generation\"]\n",
    "        \n",
    "        # Udpate prompt \n",
    "        addendum = \"\"\"  \\n --- --- --- \\n You previously tried to solve this problem. \\n Here is your solution:  \n",
    "                    \\n --- --- --- \\n {generation}  \\n --- --- --- \\n  Here is the resulting error from code \n",
    "                    execution:  \\n --- --- --- \\n {error}  \\n --- --- --- \\n Please re-try to answer this. \n",
    "                    Structure your answer with a description of the code solution. \\n Then list the imports. \n",
    "                    And finally list the functioning code block. Structure your answer with a description of \n",
    "                    the code solution. \\n Then list the imports. And finally list the functioning code block. \n",
    "                    \\n Here is the user question: \\n --- --- --- \\n {question}\"\"\"\n",
    "        template = template +  addendum\n",
    "\n",
    "        # Prompt \n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\", \"generation\", \"error\"],\n",
    "        )\n",
    "        \n",
    "        # Chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": lambda x: concatenated_content,\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"generation\": itemgetter(\"generation\"),\n",
    "                \"error\": itemgetter(\"error\"),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm_with_tool \n",
    "            | parser_tool\n",
    "        )\n",
    "\n",
    "        code_solution = chain.invoke({\"question\":question,\n",
    "                                      \"generation\":str(code_solution[0]),\n",
    "                                      \"error\":error})\n",
    "                \n",
    "    else:\n",
    "        print(\"---GENERATE SOLUTION---\")\n",
    "        \n",
    "        # Prompt \n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        )\n",
    "\n",
    "        # Chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": lambda x: concatenated_content,\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm_with_tool \n",
    "            | parser_tool\n",
    "        )\n",
    "\n",
    "        code_solution = chain.invoke({\"question\":question})\n",
    "    \n",
    "    return {\"keys\": {\"generation\": code_solution, \"question\": question}}\n",
    "\n",
    "def check_code_imports(state):\n",
    "    \"\"\"\n",
    "    Check imports\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "    \n",
    "    ## State\n",
    "    print(\"---CHECKING CODE IMPORTS---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    code_solution = state_dict[\"generation\"]\n",
    "    imports = code_solution[0].imports\n",
    "\n",
    "    try:        \n",
    "        # Attempt to execute the imports\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        # Catch any error during execution (e.g., ImportError, SyntaxError)\n",
    "        error = f\"Execution error: {e}\"\n",
    "        if \"error\" in state_dict:\n",
    "            error_prev_runs = state_dict[\"error\"]\n",
    "            error = error_prev_runs + \"\\n --- Most recent run error --- \\n\" + error     \n",
    "    else:\n",
    "        print(\"---CODE IMPORT CHECK: SUCCESS---\")\n",
    "        # No errors occurred\n",
    "        error = \"None\"\n",
    "\n",
    "    return {\"keys\": {\"generation\": code_solution, \"question\": question, \"error\": error}}\n",
    "\n",
    "def check_code_execution(state):\n",
    "    \"\"\"\n",
    "    Check code block execution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "    \n",
    "    ## State\n",
    "    print(\"---CHECKING CODE EXECUTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    code_solution = state_dict[\"generation\"]\n",
    "    imports = code_solution[0].imports\n",
    "    code = code_solution[0].code\n",
    "    code_block = imports +\"\\n\"+ code\n",
    "\n",
    "    try:        \n",
    "        # Attempt to execute the code block\n",
    "        exec(code_block)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        # Catch any error during execution (e.g., ImportError, SyntaxError)\n",
    "        error = f\"Execution error: {e}\"\n",
    "        if \"error\" in state_dict:\n",
    "            error_prev_runs = state_dict[\"error\"]\n",
    "            error = error_prev_runs + \"\\n --- Most recent run error --- \\n\" + error  \n",
    "    else:\n",
    "        print(\"---CODE BLOCK CHECK: SUCCESS---\")\n",
    "        # No errors occurred\n",
    "        error = \"None\"\n",
    "\n",
    "    return {\"keys\": {\"generation\": code_solution, \"question\": question, \"error\": error}}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "def decide_to_check_code_exec(state):\n",
    "    \"\"\"\n",
    "    Determines whether to test code execution, or re-try answer generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO TEST CODE EXECUTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    code_solution = state_dict[\"generation\"]\n",
    "    error = state_dict[\"error\"]\n",
    "\n",
    "    if error == \"None\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TEST CODE EXECUTION---\")\n",
    "        return \"check_code_execution\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def decide_to_finish(state):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO TEST CODE EXECUTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    code_solution = state_dict[\"generation\"]\n",
    "    error = state_dict[\"error\"]\n",
    "\n",
    "    if error == \"None\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TEST CODE EXECUTION---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate) # generate solution\n",
    "workflow.add_node(\"check_code_imports\", check_code_imports) # check imports\n",
    "workflow.add_node(\"check_code_execution\", check_code_execution) # check code execution\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code_imports\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code_imports\", \n",
    "    decide_to_check_code_exec,\n",
    "    {   \n",
    "    \"check_code_execution\": \"check_code_execution\", \n",
    "    \"generate\": \"generate\"},\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code_execution\", \n",
    "    decide_to_finish,\n",
    "    {   \n",
    "    \"end\": END, \n",
    "    \"generate\": \"generate\"},\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 50 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am passing text key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfoo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to my prompt and want to process it with a function, process_text(...), prior to the prompt.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[1;32m----> 4\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1333\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1332\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1335\u001b[0m     config,\n\u001b[0;32m   1336\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1337\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1338\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[0;32m   1339\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1340\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1341\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1343\u001b[0m ):\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1345\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:968\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(\n\u001b[0;32m    969\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout hitting a stop condition. You can increase the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit by setting the `recursion_limit` config key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m     )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[0;32m    975\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(read_channels(channels, output_keys))\n",
      "\u001b[1;31mGraphRecursionError\u001b[0m: Recursion limit of 50 reachedwithout hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key."
     ]
    }
   ],
   "source": [
    "question = \"I am passing text key 'foo' to my prompt and want to process it with a function, process_text(...), prior to the prompt.\"\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "app.invoke({\"keys\": {\"question\": question, \"iterations\": 0}}, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
